# Lumina-mGPT 2.0 Finetuning

For efficiency considerations, the multi-modal datasets are pre-tokenized into sequences of token ids. This leads to significantly faster training.

## Pre-tokenization


### 1. Run Tokenization

This stage tokenizes each data point, consisting of interleaved image and text, into a single sequence of integer tokens. After tokenization, the sequence is saved to disk for trainining-time usage. Together with the saved tokens, a json-formatted record file is also generated for indexing all the saved token files.

#### Command:

```bash
for i in {0..7}
do
  export CUDA_VISIBLE_DEVICES=${i}
  python -u pre_tokenize/pre_tokenize.py \
  --splits=8 \
  --rank=${i} \
  --in_filename /path/to/in_filename.json \
  --out_dir /path/to/out_dir \
  --target_size 768 &> ${i}.log &
done
```

#### Format of Input File:

`in_filename` is expected to be a json file with the following format:
```python
[
    {...},
    {...},
    {
        "conversations":[
            {
                "from": "human",
                "value": "Generate an image of {h}x{w} according to the following prompt:<|prompt|>"
            },
            {
                "from": "gpt",
                "value": "<|image|>"
            },
        ],
        "image_path": "/path/to/image1.png",
    },
    {...},
    {...}
]
```

### 2. Concat Records

After tokenization, You need to concat the record files generated by different processes (GPUs) into one single record file.


```bash
python -u pre_tokenize/concat_record.py \
--sub_record_dir /path/to/out_dir \
--save_path /path/to/out_dir/record.json
```

## Finetuning
We provide an example experiment scripts [train.sh](lumina_mgpt/scripts/train.sh) for finetuning the 7B model. 
